## Exemplar: Within-subjects experiment {#effectsize_exemplar_within} 

<mark>
This section is in *alpha*. We welcome help and feedback at all levels!
If you would like to contribute, please see
[Contributing to the Guidelines](https://github.com/transparentstats/guidelines/wiki/Contributing-to-the-Guidelines).
</mark>

Large individual differences can be a major source of noise. An effective way of accounting for that noise is for every subject to run in every combination of conditions multiple times. This "*within-subject*" experiment design combined with many repetitions per condition can substantially reduce any noise from individual differences, allowing for more precise measurements despite a small number of subjects.

In this example, we'll pretend we've run an experiment that compared different interfaces for visualizing data. Here are the parameters that we manipulate in the experiment:

* Independent Variable **layout**: the two layouts of the interface
* Independent Variable **size**: the size of the dataset visualized (small, medium, and large)
* Independent Variable **color**: interface color, where we don't expect any effect

We run each subject through each combination of these variables 20 times to get (2 layouts) × (3 sizes) × (4 colors) × (20 repetitions) = `r 2*3*4*20` trials per subject. We measure some reponse (e.g., reponse time) in each trial.


### Libraries needed for this analysis

```{r es-within-setup, warning = FALSE, message = FALSE}
library(tidyverse)
library(afex)       # for aov_ez()
library(parallel)   # for parLapply()
```

```{r es-within-boilerplate, include = FALSE}
format_num <- function(nums, sigdigits = 3) gsub("\\.$", "", formatC(nums, sigdigits, format = "fg", flag="#"))
```


### Subjects, conditions, and repetitions
In this example, there are 6 subjects (`subject` column).

```{r within-setup}
set.seed(543) # make the output consistent
SUBJECT_COUNT = 6

data <- expand.grid(
  subject = paste('Subject', LETTERS[1:SUBJECT_COUNT]), # subject IDs
  layout = 0:1, # independent variable
  size = 0:2, # independent variable
  color = 0:3, # independent variable
  repetition = 1:20 # each subject runs in each condition multiple times
)
```

### Individual differences
Not all subjects behave the same way. Some people might be tired, bad at the task, or just not trying very hard. These performance differences can't be directly measured, but they can substantially impact the results. We'll simulate these individual differences by giving each subject a random performance handicap.

```{r}
# build a table of subject performance multipliers
individualDifferences <- tibble(subject = unique(data$subject))
individualDifferences$handicap <- rnorm(SUBJECT_COUNT, 20, 4) # individual differences

# put it in the main dataset
data <- data %>% left_join(individualDifferences, by = "subject")
```


### Simulate some noisy effects
We'll simulate an experiment with a main effect of `layout` and `size` and an interaction between them. However, `color` and its interactions will not have an impact.

```{r within-simulate}

# simulate the response times with a clean model
data <- 
  data %>% 
  mutate(
  response_time = 
    layout * .4 + # main effect of layout
    size * .2 + # main effect of size
    color * 0 + 
    layout * size * .6 + # 2-way interaction
    size * color * 0 + 
    layout * color * 0 + 
    layout * size * color * 0
)

# add some reponse noise
data <- data %>% mutate(response_time = response_time + rnorm(n()))

# add noise from individual handicaps
data <- data %>% mutate(response_time = 30 + handicap*2 + response_time * handicap)

```

Even though we used numbers to simulate the model, the independent variables and subject ID are all factors.
```{r within-factor}
data <- 
  data %>% 
  mutate(
    subject = factor(subject), 
    layout = factor(layout), 
    color = factor(color)
  )
```

### A look at the data

Let's get an overview of the results by graphing each subject's average response time for each condition.

```{r within-sneak-peak}
data %>% 
  group_by(layout, size, color, subject) %>% 
  summarize(response_time = mean(response_time)) %>% 
ggplot() + 
  aes(y=response_time, x=size, linetype=layout, color=color, 
      group=paste(layout,color,subject)) + 
  geom_line(size=1.5) + 
  scale_color_brewer(palette='Set1') + 
  facet_wrap(~subject) + 
  labs(title='Response times for each subject') +
  theme_bw()
```

Despite a lot of variability in raw values between subjects (individual differences), we can see some consistent patterns. The dashed lines are higher (main effect) and more sloped (interaction) than the solid lines. But there doesn't seem to be any consistent ordering of the colors.


### Compute effect sizes
While **Cohen's *d* ** is often used for simple 2-factor, single-trial, between-subject designs, repetition skews the measure to be very high. Experiment results with lots of repetition can be more reliably interpretted with the **eta squared ($\eta^{2}$)** family of effect sizes, which represent the proportion of variance accounted for by a particular variable. A variant, **generalized eta squared ($\eta_{G}^{2}$)**, is particularly suited for providing comparable effect sizes in both between and within-subject designs [@Olejnik2003; @Bakeman2005]. This property makes it more easily applicable to meta-analyses.

For those accustomed to Cohen's *d*, it's important to be aware that $\eta_{G}^{2}$ is typically smaller, with a Cohen's d of 0.2 being equivalent to a $\eta_{G}^{2}$ of around 0.02. Also, the actual number has little meaning beyond its scale relative to other effects. 

```{r within-anova}
results = afex::aov_ez(
  data = data, 
  id = 'subject', # subject id column
  dv = 'response_time', # dependent variable
  within = c('layout', 'size', 'color'), # within-subject independent variables
  between = NULL ,# between-subject independent variables
  fun_aggregate = mean, # average multiple repetitions together for each subject*condition
  anova_table = list(es = 'ges') # effect size = generalized eta squared
)
```

*Note: `fun_aggregate = mean` collapses repetitions into a mean, which may be a problem if an experiment is not fully counterbalanced. This example, however, has every subject running in every combination of conditions, so simple collapsing is the correct procedure.*

```{r  within-anova-cleanup}
anovaResults <- 
  results$anova_table %>% 
  rownames_to_column('effect') %>%  # put effect names in a column
  select(-`Pr(>F)`) # no need to show p-values
  
anovaResults %>% as.tibble()
```

Looking at the `F` and `ges` (generalized eta squared) columns, there appear to be strong main effects for `layout` and `size` and an interaction between `layout` and `size`. However `color` and the other 2-way and 3-way interactions show only negligeable effects.



### Bootstrapping

<mark> Draft. Needs work. </mark>

But that only gives us one value, whereas we want a confidence interval.

We'll use a technique called bootstrapping, which checks the effect size for many random permutations of the data.

1. Randomly sample with replacement (meaning the same value might be sampled more than once) to build a new dataset
1. Perform the analysis on this new dataset
1. Do that many times
1. Sort the results for each effect and find the 95% confidence interval


#### Each iteration

Each iteration of the bootstrap samples the original dataset and runs the analysis on this permutation.

```{r within-bootstrap-by-subject}

# run one iteration of the bootstrap procedure
analyzeIteration <- function(x) {
  subjects <- unique(dataAggregated$subject)
  
  # select subjects at random with replacement
  bootSubjects <- sample(subjects, length(subjects), replace=TRUE)
  
  # get all the results for one subject
  # and give them a new uniwue subject id
  getOneSubjectsData <- function(i) {
    dataAggregated %>% 
      filter(subject == bootSubjects[i]) %>% 
      mutate(subject = paste(bootSubjects[i], i))
  }
  
  # get all of the boostrapped subjects' data
  bootData <- lapply(1:length(bootSubjects), getOneSubjectsData) %>% 
    bind_rows()
  
  # compute the effect sizes the same way we did without bootstrapping
  afex::aov_ez(
    data = bootData, 
    id = 'subject', # subject id column
    dv = 'response_time', # dependent variable
    within = c('layout', 'size', 'color'), # within-subject independent variables
    between = NULL ,# between-subject independent variables
    #fun_aggregate = mean,
    anova_table = list(es = 'ges') # effect size = generalized eta squared
  )$anova_table %>% 
    as.tibble() %>% 
    rownames_to_column('effect') %>% # put effect names in a column
    return()
}
```

#### Iterate

The bootstrap needs to run many times to determine how a subset of the data impacts

```{r within-bootstrap-bootstrap iterator}

# run many iterations of the bootstrap procedure
analyzeManyIterations = function (bootstrapIterationCount) {
  # each core needs to reload the libraries
  library(tidyverse)
  library(afex)
  lapply(1:bootstrapIterationCount, function(x) analyzeIteration(x)) %>% 
    bind_rows()
}

# data used for bootstrapping will collapse by each subject-condition combination
dataAggregated <- data %>% 
  group_by(layout, size, color, subject) %>% 
  summarize(response_time = mean(response_time))

```


#### Parallelize

Bootstrapping can be slow, especially with thousands of iterations. Splitting the iterations across processor cores cuts down on the wait time.

```{r within-parallelize-boostrap, cache=TRUE}
BOOTSTRAP_COUNT <- 100 # at least 5k recommended. Use lower values for quicker testing

# Initiate cluster
coreCount <- detectCores() - 1
ifelse(coreCount < 1, 1, coreCount)
cl <- makeCluster(coreCount)
clusterExport(cl, "analyzeIteration")
clusterExport(cl, "dataAggregated")
# how many times should each core iterate 
bootstrapIterationCount <- BOOTSTRAP_COUNT / coreCount
# run the bootstrap and output the time
system.time(
  bootResults <- parLapply(cl, rep(bootstrapIterationCount, coreCount), analyzeManyIterations)
)
# the cluster is no longer needed
stopCluster(cl)
# cleanup
rm(coreCount, cl, bootstrapIterationCount, dataAggregated)
```

### Getting a confidence interval from a bootstrap

Each bootstrap iterations ran one anaylsis, so wwe now have many results. So for each effect size, we sort the results and find the range of the inner 95%.

```{r within-boostrap-3}
# inner 95%
PERCENTILE_LO <- 0.025
PERCENTILE_HI <- 0.975

# put all the boostraped results together
bootResults <- bind_rows(bootResults)
bootResults <- 
  bootResults %>% 
  group_by(effect) %>% 
  summarize(
    ESPercentileLo = unname(quantile(ges, probs = PERCENTILE_LO)),
    ESPercentileHi = unname(quantile(ges, probs = PERCENTILE_HI)))

anovaResults <- 
  anovaResults %>% 
  left_join(bootResults, by = 'effect')
anovaResults %>% as.tibble()
  
ggplot(anovaResults) +
  aes(y=forcats::fct_rev(forcats::fct_inorder(effect)), x=ges) +
  geom_vline(xintercept = 0, linetype = 'dotted') +
  geom_segment(aes(x=ESPercentileLo, xend=ESPercentileHi, yend=effect), size=1) +
  geom_point(size=3) +
  scale_x_continuous(expand = c(0,0)) + expand_limits(x = -0.05) +
  expand_limits(x = max(anovaResults$ESPercentileHi) * 1.05) +
  labs(x=expression(paste('Effect size ', eta[G]^2)), y='Effect') +
  theme_bw()
```


### Reporting the results

Generalized eta squared (GES) represents the proportion of variance in the results explained by each variable. The previous graph shows clear main effects for `layout` and `size` and an interaction between `layout` and `size`. However `color` and the other 2-way and 3-way interactions are relatively much smaller, barely above zero. There is no useful cutoff for what counts as a "significant" effect, so think in terms of relative size -- which variables best explain the variance in the results? 

```{r within-format, include=FALSE}
# format the anova results for a report, and trim to 3 significant digits
formatGES <- function(anovaTable, effectName) {
  cutoff = 0.01
  row = (1:nrow(anovaTable))[anovaTable$effect == effectName]
  return(paste0(
    'F~',
    signif(anovaTable[row, 'num Df'], 3), ',',
    signif(anovaTable[row, 'den Df'], 3), '~ = ',
    signif(anovaTable[row, 'F'], 3), ', $\\eta_{G}^{2}$ = ',
    signif(anovaTable[row, 'ges'], 3), ' 95% CI [',
    signif(anovaTable[row, 'ESPercentileLo'], 3), ', ',
    signif(anovaTable[row, 'ESPercentileHi'], 3), ']'
  ))
}
```


Strong effects:

 - **layout:** `r formatGES(anovaResults, 'layout')`
 - **size:** `r formatGES(anovaResults, 'size')`
 - **layout** × **size:** `r formatGES(anovaResults, 'layout:size')`
 
Minimally impactful:

 - **color** `r formatGES(anovaResults, 'color')`
 - **layout** × **color:** `r formatGES(anovaResults, 'layout:color')`
 - **size** × **color:** `r formatGES(anovaResults, 'size:color')`
 - **layout** × **size** × **color:** `r formatGES(anovaResults, 'layout:size:color')`

